---
tittle:吴恩达机器学习-神经网络
tag:神经网络
---

## 1. 神经网络的直觉理解

为了清楚地说明什么是神经网络，我们先从一个简单的例子开始。

 ![sigmoid函数]({{ site.baseurl }}/assets/img/neural_network/XOR函数.png) 

上图展示了XOR/XNOR函数的分类问题？那么我们应该如何使用神经网络算法来表示XOR/XNOR函数模型呢？

事实上，神经网络就是多层逻辑回归的嵌套，根据化繁为简的原则，我们先从建单的AND函数说起

![sigmoid函数]({{ site.baseurl }}/assets/img/neural_network/AND函数.png) 

我们先暂时放下神经网络，回到我们之前说过的逻辑函数，如何使用逻辑回归来实现一个AND函数的功能？上图给出了使用逻辑回归解决AND函数的问题。根据图示，我们有两个未知变量$$x_1,x_2$$和一个截距项，根据经验我们设置这个截距项为1，那么我们的假设函数可以写成$$h_\theta(x)=g(-30+20x_1+20x_2)$$(这只是举例说明一种可能的情况，真实的参数不一定是这样)，其中$$g(z)=\frac{1}{1+e_{-z}}$$。从右下角的表格中我们可以看到当两个位置变量$$x_1,x_2$$分别为0,0时，$$h(x)=g(-30)\approx0$$；当两个变量分别为0,1时$$h(x)=g(-10)\approx0$$ ；当两个变量分别为1,0时，$$h(x)=g(-10)\approx0$$ ；当两个变量分别为1,1时，$$h(x)=g(10)\approx1$$ 。（补充：在逻辑回归中，我们认为当z>0时，g(z)= 1）。这样我们就使用逻辑回归表示了函数AND的功能。

上面我们说明了使用逻辑回归表示AND函数的功能，你能自己证明如何使用逻辑回归表示OR函数和(NOT $$x_1$$) AND (NOT $$x_2$$)函数吗？

下面我们直接说明XNOR函数如何实现

![sigmoid函数]({{ site.baseurl }}/assets/img/neural_network/XNOR函数.png)

如上图所示，神经网络的输入层为+1，$$x_1,x_2$$,其中+1为我们的惩罚项，我们让隐藏层的第一个参数$$a_1^{(2)}$$实现AND函数，参数分别为[-30,20,20]，让隐藏层的第二个参数$$a_2^{(2)}$$实现(NOT $$x_1$$) AND (NOT $$x_2$$)函数，参数为[10,-20,-20]，再加上隐藏层的惩罚项，我们让输出层实现OR函数，参数为[-10,20,20]。这样，经过输入层、隐藏层和输出层的处理后，我们就使用神经网络实现了XNOR函数

以上就是我们对神经网络的直觉理解即：神经网络是由多个嵌套的逻辑回归实现的

## 2. 神经网络实现多元分类

神经网络的多元分类可以看成是二元分类的扩展

![sigmoid函数]({{ site.baseurl }}/assets/img/neural_network/神经网络多元分类.png)

在我们预测二元回归时，我们直接输出了0或者1的结果。在处理多元分类问题时，我们将结果y表示为一个k维（k>=3）向量。以上图为例说明多元分类问题，上图中我们将结果分为行人、汽车、摩托车、卡车四中类型。当结果为行人时我们将其表示为[1,0,0,0],当结果为汽车时我们将其表示为[0,1,0,0]，当结果为摩托车时我们将其表示为[0,0,1,0]，当结果为卡车时我们将其表示为[0,0,01]。这样当我们输入一个输入变量时就会得到一个k维的向量作为输出，数值最大的那个类就是我们的预测结果。

## 3. 神经网络的代价函数

![sigmoid函数]({{ site.baseurl }}/assets/img/neural_network/代价函数.png)

上图给出了神经网络的代价函数，应为神经网络实质上市逻辑回归的嵌套，因此神经网络的代价函数也就是各个逻辑回归代价函数的累加和。代价函数的最后一项是正则化项，它可以防止过拟合的情况发生，通常情况下，我们的正则化项不计算第一项的值。

## 4. 反向传播算法

