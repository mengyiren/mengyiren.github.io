---
title: 吴恩达机器学习-正则化
tags: 机器学习 正则化 笔记
---

## 正则化

### 1. 正则化要解决的问题

首先明确正则化要解决的问题，正则化要解决的是过拟合的问题，我们以下图为例说来说明

 ![sigmoid函数]({{ site.baseurl }}/assets/img/regularization/过拟合.png) 

上图展示了关于房价预测的函数模型，第一幅图的假设函数是线性函数$$h(x)=\theta_0+\theta_1 x$$ ，第二幅图的假设函数是二次函数$$h(x)=\theta_0+\theta_1 x+\theta_2 x^2$$ ，第三幅图的假设函数是高阶函数$$h(x)=\theta_0+\theta_1 x+\theta_2 x^2+\theta_3 x^3+\theta_4 x^4$$ 。我们可以看到第一幅图对结果拟合不太令人满意，它并没有包含全部的训练数据，这种现象称为欠拟合。第二幅图中的预测函数平滑地通过了所有的训练样本，我们认为这样的预测函数符合我们的要求。第三幅图虽然也通过了所有的训练样本，但是因为假设模型是高阶函数因此函数的曲线没有二次函数光滑，并且函数的实际预测效果没有第二幅图的预测效果好，因为我们认为相邻很近的两个点的预测结果应该是相似的，但是第三幅图中出现了很过的拐点，这让假设函数的预测效果大大降低，这种现象称为过拟合，过拟合也叫高方差。

为了解决过拟合问题，我们可以可以将第三幅图中的参数$$\theta_3$$ 与$$\theta_4$$ 的值无限接近于0，这样一来第三幅图的假设函数就近似为$$h(x)\approx\theta_0+\theta_1 x+\theta_2 x^2$$ 此时，函数模型就近似于二次函数，假设函数模型的预测意义就提高了，这就是正则化的思想



## 2. 代价函数

正则化的思想我们已经说明白了，那么如何使用正则化的思想来优化我们的算法呢？

![sigmoid函数]({{ site.baseurl }}/assets/img/regularization/代价函数.png) 

上图展示了正则化如何优化我们的算法，其中的文字部分是对我们正则化思想的说明，即1、通过正则化我们可以拟合出更简单的假设函数模型；2、我们可以防止过拟合问题的出现。图中举了房价预测的例子来说明如何将正则化技术推广到更一般的情况来防止过拟合：假如影响我们房价的特征向量有100个，那么我们房价预测的模型就可能是非常复杂的高阶非线性模型，这时我们应该选择哪些特征值，通过减小他们的参数值来简化模型呢？我相信这个问题很难回答，那么我们就将所有的参数都减小，但是通常我们不会减小$$\theta_0$$的值，事实上，即使减小了$$\theta_0$$的值也不会对实际结果产生很大影响。那么为了减小所有的参数值，我们就得出了上图中正则化的代价函数。图中的代价函数的公式告诉我们正则化技术应该如何指导我们的预测模型：我们可以在代价函数后面加上正则化项$$\lambda\sum_{i=1}^m\theta_j^2$$ ，其中$$\lambda$$代表正则化系数，$$\theta_j^2$$代表我们第j个参数值的平方。

也许你会有一个疑惑为什么在代价函数的末尾加上一个正则化项就能减小所有参数的值呢？在这里我给出我自己的看法，如果你有不同的意见可以点击本文最下方Github的链接到我的Github发信息告诉我你的观点，欢迎随时提出自己的观点

那么，我们继续回来说明为什么在代价函数上加一个正则项就可以减小所有参数的值。我将通过数学公式的方式来说明。大部分对参数的求解算法都会使用到梯度下降算法，梯度算法的公式如下：$$\theta:=\theta+\alpha\Delta_\theta J(\theta)$$。我们以线性回归为例说明，线性回归梯度下降公式为：$$\theta_j:=\theta_j -\frac{1}{m}\sum_{i=1}^m\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} -\frac{\alpha\lambda}{m}\theta_j$$

在梯度下降算法中我们会用到代价函数的偏导数来求解我们的参数$$\theta$$，因为正则化项中包含$$\theta_j$$所以在对参数$$\theta_j$$求导的时候正则化项不会消除，相反正则化项会在梯度下降公式中额外增加一项，前面的$$\theta_j$$减去这一项的参数为($$1-\alpha\frac{\lambda}{m}$$)，我们可以直观地看出这个系数小于1，这个系数就减小了所有参数$$\theta$$的值



## 3. 线性回归的正则化

上文已经给出了线性回归的梯度下降正则化公式，我就原样照搬过来，至于梯度下降的推导过程这里我就不再证明，相信你自己一定可以证明出来。

代价函数：$$J(\theta)=\frac{1}{2m}(\sum_{(i=1)}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2)$$

梯度下降函数：$$\theta_j:=(1-\frac{\alpha\lambda}{m})\theta_j -\frac{1}{m}\sum_{i=1}^m\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} $$



## 4. 逻辑回归的正则化

逻辑回归的正则化与线性回归的正则化类似也需要在代价函数中增加一个正则化项

代价函数：$$J(\theta)=\sum_{i=0}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$

梯度下降函数：$$\theta_j:=(1-\frac{\alpha\lambda}{m})\theta_j -\frac{1}{m}\sum_{i=1}^m\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} $$



## 5. 总结

现在回答在逻辑回归中提出的问题，线性回归的梯度下降算法和逻辑回归中的梯度下降算法（也有人叫梯度上升算法）不是同一个算法，因为算法中的$$h_\theta(x)$$不是同一个函数，在线性回归中$$h_\theta(x)$$是线性函数，而在逻辑回归中$$h_\theta(x)$$是sigmoid函数